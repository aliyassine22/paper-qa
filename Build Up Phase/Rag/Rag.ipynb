{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34a59f2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824f20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Optional, Sequence, Type, Union, Annotated\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.conversational_retrieval.base import BaseConversationalRetrievalChain\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from pydantic import BaseModel, Field \n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.tools import BaseTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64f97e",
   "metadata": {},
   "source": [
    "# Building the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03325281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def educated_retriever(\n",
    "    llm: BaseLanguageModel,\n",
    "    metadata_field_info: Sequence[Union[AttributeInfo, dict]],\n",
    "    document_content: str,\n",
    "    vectordb: VectorStore,\n",
    "    chain_type: str = \"stuff\",\n",
    ") -> BaseConversationalRetrievalChain:\n",
    "    \"\"\"\n",
    "    Builds a conversational retrieval QA pipeline by combining a SelfQueryRetriever\n",
    "    with a ConversationalRetrievalChain.\n",
    "    \"\"\"\n",
    "    retriever = SelfQueryRetriever.from_llm(\n",
    "        llm,\n",
    "        vectordb,\n",
    "        document_contents=document_content,\n",
    "        metadata_field_info=metadata_field_info,\n",
    "        verbose=True,\n",
    "    )\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=chain_type,\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "122ea834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSearchInput(BaseModel):\n",
    "    \"\"\"Input schema for the RAG search tool\"\"\"\n",
    "    query: str = Field(..., description=\"The query to search for in the research papers\")\n",
    "    papers_path: Optional[str] = Field(default=None, description=\"Path to the research papers directory\")\n",
    "    subject: Optional[str] = Field(default=None, description=\"Subject area (e.g., 'Artificial Intelligence')\")\n",
    "    topic: Optional[str] = Field(default=None, description=\"Topic within the subject (e.g., 'Agentic AI', 'Finetuning')\")\n",
    "    year: Optional[int] = Field(default=None, description=\"Publication year of the paper\")\n",
    "    chain_type: Optional[str] = Field(default=\"stuff\", description=\"Chain type for the QA system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd168e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSearchTool(BaseTool):\n",
    "    name: Annotated[str, Field(description=\"Name of the tool\")] = \"research_paper_search\"\n",
    "    description: Annotated[str, Field(description=\"Description of the tool\")] = \"\"\"\n",
    "    Search through research papers using an educated RAG approach.\n",
    "    Uses metadata filtering (subject, topic, year, paper_title) and conversational capabilities \n",
    "    to provide relevant information from AI research papers.\n",
    "    \"\"\"\n",
    "    args_schema: Type[BaseModel] = RAGSearchInput\n",
    "    default_papers_path: Path\n",
    "    persist_directory: Path\n",
    "    collection_name: str = \"research_papers\"\n",
    "\n",
    "    _qa_chain: Any = None\n",
    "    _vectordb: Any = None\n",
    "    _embeddings: Any = None\n",
    "    _init_lock: Any = None\n",
    "    _llm: Any = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        default_papers_path: Optional[Path] = None,\n",
    "        persist_directory: Optional[Path] = None,\n",
    "        collection_name: str = \"research_papers\",\n",
    "    ):\n",
    "        # First, determine the final paths for the required fields.\n",
    "        base_dir = Path(os.getcwd()).resolve().parent.parent.parent.parent\n",
    "        final_papers_path = default_papers_path or base_dir / \"Build Up Phase\" / \"Rag\" / \"Papers\"\n",
    "        final_persist_dir = Path(persist_directory or base_dir / \"Build Up Phase\" / \"Rag\" / \"VectorDB\").resolve()\n",
    "        super().__init__(\n",
    "            default_papers_path=final_papers_path,\n",
    "            persist_directory=final_persist_dir,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "\n",
    "        self._embeddings = OpenAIEmbeddings()\n",
    "        self._init_lock = threading.Lock()\n",
    "\n",
    "    def _extract_paper_metadata(self, file_path: Path, base_path: Path) -> dict:\n",
    "        \"\"\"\n",
    "        Extract metadata from research paper filename and path.\n",
    "        Expected filename format: {paper_title} - {year} - {description}.pdf\n",
    "        Expected path structure: Papers/Subject/Topic/filename.pdf\n",
    "        \"\"\"\n",
    "        metadata = {\n",
    "            \"subject\": \"Artificial Intelligence\",  # Default for now\n",
    "            \"topic\": None,\n",
    "            \"paper_title\": None,\n",
    "            \"year\": None,\n",
    "            \"file_name\": file_path.name,\n",
    "            \"file_path\": str(file_path),\n",
    "        }\n",
    "        \n",
    "        # Extract subject and topic from directory structure\n",
    "        try:\n",
    "            rel_path = file_path.relative_to(base_path)\n",
    "            parts = rel_path.parts\n",
    "            # Expected: Subject/Topic/filename.pdf\n",
    "            if len(parts) >= 2:\n",
    "                metadata[\"subject\"] = parts[0]  # e.g., \"Artificial Intelligence\"\n",
    "            if len(parts) >= 3:\n",
    "                metadata[\"topic\"] = parts[1]  # e.g., \"Agentic AI\", \"Finetuning\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # Parse filename: {title} - {year} - {description}.pdf\n",
    "        filename_stem = file_path.stem  # Remove .pdf extension\n",
    "        \n",
    "        # Split by \" - \" to get parts\n",
    "        parts = [p.strip() for p in filename_stem.split(\" - \")]\n",
    "        \n",
    "        if len(parts) >= 1:\n",
    "            metadata[\"paper_title\"] = parts[0]\n",
    "        \n",
    "        if len(parts) >= 2:\n",
    "            # Try to extract year from second part\n",
    "            year_match = re.search(r\"(\\d{4})\", parts[1])\n",
    "            if year_match:\n",
    "                metadata[\"year\"] = int(year_match.group(1))\n",
    "            else:\n",
    "                # If no year in second part, it might be part of title or description\n",
    "                metadata[\"paper_title\"] = f\"{parts[0]} - {parts[1]}\"\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    def _load_or_build_vectordb(self):\n",
    "        db_file = self.persist_directory / \"chroma.sqlite3\"\n",
    "        if db_file.exists() and self._vectordb is None:\n",
    "            self._vectordb = Chroma(\n",
    "                persist_directory=str(self.persist_directory),\n",
    "                collection_name=self.collection_name,\n",
    "                embedding_function=self._embeddings,\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if self._vectordb is not None:\n",
    "            return\n",
    "\n",
    "        loader = DirectoryLoader(\n",
    "            str(self.default_papers_path),\n",
    "            glob=\"**/*.pdf\",\n",
    "            loader_cls=PyMuPDFLoader,\n",
    "        )\n",
    "        raw_docs = loader.load()\n",
    "\n",
    "        for d in raw_docs:\n",
    "            p = Path(d.metadata[\"source\"])\n",
    "            \n",
    "            # Extract research paper metadata\n",
    "            paper_metadata = self._extract_paper_metadata(p, self.default_papers_path)\n",
    "            d.metadata.update(paper_metadata)\n",
    "            \n",
    "            # Add additional useful metadata\n",
    "            d.metadata.update({\n",
    "                \"doc_id\": p.stem,\n",
    "                \"relpath\": str(p.relative_to(self.default_papers_path)) if self.default_papers_path in p.parents or p.parent == self.default_papers_path else p.name\n",
    "            })\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=250)\n",
    "        split_docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "        self.persist_directory.mkdir(parents=True, exist_ok=True)\n",
    "        self._vectordb = Chroma.from_documents(\n",
    "            documents=split_docs,\n",
    "            embedding=self._embeddings,\n",
    "            collection_name=self.collection_name,\n",
    "            persist_directory=str(self.persist_directory),\n",
    "        )\n",
    "\n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize the RAG components if not already initialized.\"\"\"\n",
    "        if self._qa_chain is not None:\n",
    "            return\n",
    "        with self._init_lock:\n",
    "            if self._qa_chain is not None:\n",
    "                return\n",
    "            self._load_or_build_vectordb()\n",
    "            metadata_field_info = [\n",
    "                AttributeInfo(\n",
    "                    name=\"subject\", \n",
    "                    type=\"string\", \n",
    "                    description=\"Subject area of the research paper (e.g., 'Artificial Intelligence', 'Communication Systems', 'Security')\"\n",
    "                ),\n",
    "                AttributeInfo(\n",
    "                    name=\"topic\", \n",
    "                    type=\"string\", \n",
    "                    description=\"Specific topic within the subject (e.g., 'Agentic AI', 'Finetuning', 'Hierarchical Reasoning Models')\"\n",
    "                ),\n",
    "                AttributeInfo(\n",
    "                    name=\"paper_title\", \n",
    "                    type=\"string\", \n",
    "                    description=\"Title of the research paper\"\n",
    "                ),\n",
    "                AttributeInfo(\n",
    "                    name=\"year\", \n",
    "                    type=\"integer\", \n",
    "                    description=\"Publication year of the research paper\"\n",
    "                ),\n",
    "                AttributeInfo(\n",
    "                    name=\"file_name\", \n",
    "                    type=\"string\", \n",
    "                    description=\"The filename of the PDF document\"\n",
    "                ),\n",
    "            ]\n",
    "            self._llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "            self._qa_chain = educated_retriever(\n",
    "                llm=self._llm,\n",
    "                metadata_field_info=metadata_field_info,\n",
    "                document_content=\"Research papers on Artificial Intelligence topics including Agentic AI, Finetuning, and Hierarchical Reasoning Models\",\n",
    "                vectordb=self._vectordb,\n",
    "                chain_type=\"stuff\",\n",
    "            )\n",
    "\n",
    "    def _build_metadata_filter(self, subject: Optional[str], topic: Optional[str], year: Optional[int]) -> Optional[dict]:\n",
    "        \"\"\"\n",
    "        Build a Chroma-compatible metadata filter from the provided parameters.\n",
    "        Uses $and to combine multiple conditions.\n",
    "        \"\"\"\n",
    "        conditions = []\n",
    "        \n",
    "        if subject:\n",
    "            conditions.append({\"subject\": {\"$eq\": subject}})\n",
    "        if topic:\n",
    "            conditions.append({\"topic\": {\"$eq\": topic}})\n",
    "        if year:\n",
    "            conditions.append({\"year\": {\"$eq\": year}})\n",
    "        \n",
    "        if not conditions:\n",
    "            return None\n",
    "        elif len(conditions) == 1:\n",
    "            return conditions[0]\n",
    "        else:\n",
    "            return {\"$and\": conditions}\n",
    "\n",
    "    def _generate_answer_from_docs(self, query: str, docs: List) -> str:\n",
    "        \"\"\"Generate an answer from retrieved documents using the LLM.\"\"\"\n",
    "        if not docs:\n",
    "            return \"No documents found matching the specified filters.\"\n",
    "        \n",
    "        context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        )\n",
    "        \n",
    "        chain = prompt | self._llm\n",
    "        response = chain.invoke({\"context\": context, \"question\": query})\n",
    "        return response.content.strip()\n",
    "\n",
    "    def _format_sources(self, docs: List) -> List[str]:\n",
    "        \"\"\"Format source documents into a list of source strings.\"\"\"\n",
    "        sources = []\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            md = doc.metadata or {}\n",
    "            parts = [f\"[{i}]\"]\n",
    "            \n",
    "            if md.get(\"paper_title\"):\n",
    "                parts.append(f\"'{md.get('paper_title')}'\")\n",
    "            if md.get(\"year\"):\n",
    "                parts.append(f\"({md.get('year')})\")\n",
    "            if md.get(\"topic\"):\n",
    "                parts.append(f\"[{md.get('topic')}]\")\n",
    "            if md.get(\"subject\"):\n",
    "                parts.append(f\"- {md.get('subject')}\")\n",
    "            \n",
    "            page = md.get(\"page\")\n",
    "            if page is not None:\n",
    "                parts.append(f\"p.{page + 1}\")\n",
    "            \n",
    "            sources.append(\" \".join(parts))\n",
    "        return sources\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        query: str,\n",
    "        papers_path: Optional[str] = None,\n",
    "        subject: Optional[str] = None,\n",
    "        topic: Optional[str] = None,\n",
    "        year: Optional[int] = None,\n",
    "        k: int = 10,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Executes the RAG search by applying explicit metadata filters\n",
    "        to ensure accurate filtering by subject, topic, and year.\n",
    "        When explicit filters are provided, uses direct vector store retrieval\n",
    "        instead of SelfQueryRetriever to guarantee filter accuracy.\n",
    "        \"\"\"\n",
    "        if papers_path:\n",
    "            new_path = Path(papers_path)\n",
    "            if self.default_papers_path != new_path:\n",
    "                self.default_papers_path = new_path\n",
    "                self._qa_chain = None\n",
    "                self._vectordb = None\n",
    "                if self.persist_directory.exists():\n",
    "                    shutil.rmtree(self.persist_directory)\n",
    "\n",
    "        self._initialize_components()\n",
    "\n",
    "        # Build explicit metadata filter for Chroma\n",
    "        metadata_filter = self._build_metadata_filter(subject, topic, year)\n",
    "\n",
    "        try:\n",
    "            # If explicit filters are provided, bypass SelfQueryRetriever\n",
    "            # and use direct vector store retrieval with guaranteed filters\n",
    "            if metadata_filter:\n",
    "                # Use direct similarity search with explicit filter\n",
    "                docs = self._vectordb.similarity_search(\n",
    "                    query,\n",
    "                    k=k,\n",
    "                    filter=metadata_filter\n",
    "                )\n",
    "                \n",
    "                answer = self._generate_answer_from_docs(query, docs)\n",
    "                sources = self._format_sources(docs)\n",
    "                \n",
    "                return f\"Answer: {answer}\\n\\nSources:\\n\" + (\"\\n\".join(sources) if sources else \"(none)\")\n",
    "            \n",
    "            else:\n",
    "                # No explicit filters - use the SelfQueryRetriever for intelligent querying\n",
    "                retr = self._qa_chain.retriever\n",
    "                orig_kwargs = retr.search_kwargs.copy()\n",
    "                try:\n",
    "                    retr.search_kwargs[\"k\"] = k\n",
    "                    response = self._qa_chain({\"question\": query, \"chat_history\": []})\n",
    "\n",
    "                    answer = (response.get(\"answer\") or \"\").strip()\n",
    "                    sources = self._format_sources(response.get(\"source_documents\", []) or [])\n",
    "\n",
    "                    return f\"Answer: {answer}\\n\\nSources:\\n\" + (\"\\n\".join(sources) if sources else \"(none)\")\n",
    "                finally:\n",
    "                    if self._qa_chain:\n",
    "                        self._qa_chain.retriever.search_kwargs = orig_kwargs\n",
    "                        \n",
    "        except Exception as e:\n",
    "            return f\"Error: {e!r}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a5e32",
   "metadata": {},
   "source": [
    "# Trying the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fcb2c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG tool initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG tool with explicit paths for notebook usage\n",
    "papers_path = Path(r\"c:\\Users\\User\\Desktop\\llms\\Project\\Research Assistant Multi Agent System\\Build Up Phase\\Rag\\Papers\")\n",
    "vectordb_path = Path(r\"c:\\Users\\User\\Desktop\\llms\\Project\\Research Assistant Multi Agent System\\Build Up Phase\\Rag\\VectorDB\")\n",
    "\n",
    "rag_tool = RAGSearchTool(\n",
    "    default_papers_path=papers_path,\n",
    "    persist_directory=vectordb_path,\n",
    "    collection_name=\"research_papers\"\n",
    ")\n",
    "print(\"RAG tool initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fd85b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: QLoRA is an efficient fine-tuning approach designed to reduce memory usage, enabling the fine-tuning of large language models (LLMs) with billions of parameters on consumer-grade GPUs. Specifically, it allows for the fine-tuning of a 65 billion parameter model on a single 48GB GPU while maintaining performance comparable to full 16-bit fine-tuning. QLoRA achieves this by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).\n",
      "\n",
      "The method introduces several innovations to save memory without sacrificing performance, including:\n",
      "\n",
      "1. **4-bit NormalFloat (NF4)**: A new data type that is optimal for normally distributed weights.\n",
      "2. **Double Quantization**: This technique reduces the average memory footprint by quantizing the quantization constants.\n",
      "3. **Paged Optimizers**: These help manage memory spikes during the training process.\n",
      "\n",
      "QLoRA has been shown to outperform previous models on benchmarks like Vicuna, achieving high performance with significantly lower resource requirements. This makes fine-tuning more accessible, particularly for researchers with limited resources, and opens up possibilities for deploying LLMs in low-resource settings, such as mobile devices.\n",
      "\n",
      "Sources:\n",
      "[1] 'QLoRA, Efficient Finetuning of Quantized LLMs' (2023) [Finetuning] - Artificial Intelligence p.1\n",
      "[2] 'QLoRA, Efficient Finetuning of Quantized LLMs' (2023) [Finetuning] - Artificial Intelligence p.16\n",
      "[3] 'QLoRA, Efficient Finetuning of Quantized LLMs' (2023) [Finetuning] - Artificial Intelligence p.16\n",
      "[4] 'QLoRA, Efficient Finetuning of Quantized LLMs' (2023) [Finetuning] - Artificial Intelligence p.22\n",
      "[5] 'QLoRA, Efficient Finetuning of Quantized LLMs' (2023) [Finetuning] - Artificial Intelligence p.7\n",
      "[6] 'QLoRA, Efficient Finetuning of Quantized LLMs' (2023) [Finetuning] - Artificial Intelligence p.8\n",
      "[7] 'QLoRA, Efficient Finetuning of Quantized LLMs' (2023) [Finetuning] - Artificial Intelligence p.6\n",
      "[8] 'QLoRA, Efficient Finetuning of Quantized LLMs' (2023) [Finetuning] - Artificial Intelligence p.16\n",
      "[9] 'QLoRA, Efficient Finetuning of Quantized LLMs' (2023) [Finetuning] - Artificial Intelligence p.14\n",
      "[10] 'QLoRA, Efficient Finetuning of Quantized LLMs' (2023) [Finetuning] - Artificial Intelligence p.15\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Filter by topic - Finetuning\n",
    "result = rag_tool._run(\"What is QLoRA and how does it help with fine-tuning LLMs?\", topic=\"Finetuning\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feb1aecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I don't know.\n",
      "\n",
      "Sources:\n",
      "[1] 'AI Agents vs Agentic AI, A Conceptual Taxonomy Applications and Challenges' (2025) [Agentic AI] - Artificial Intelligence p.27\n",
      "[2] 'AI Agents vs Agentic AI, A Conceptual Taxonomy Applications and Challenges' (2025) [Agentic AI] - Artificial Intelligence p.1\n",
      "[3] 'AI Agents vs Agentic AI, A Conceptual Taxonomy Applications and Challenges' (2025) [Agentic AI] - Artificial Intelligence p.3\n",
      "[4] 'AI Agents vs Agentic AI, A Conceptual Taxonomy Applications and Challenges' (2025) [Agentic AI] - Artificial Intelligence p.24\n",
      "[5] 'AI Agents vs Agentic AI, A Conceptual Taxonomy Applications and Challenges' (2025) [Agentic AI] - Artificial Intelligence p.3\n",
      "[6] 'AI Agents vs Agentic AI, A Conceptual Taxonomy Applications and Challenges' (2025) [Agentic AI] - Artificial Intelligence p.3\n",
      "[7] 'AI Agents vs Agentic AI, A Conceptual Taxonomy Applications and Challenges' (2025) [Agentic AI] - Artificial Intelligence p.27\n",
      "[8] 'AI Agents vs Agentic AI, A Conceptual Taxonomy Applications and Challenges' (2025) [Agentic AI] - Artificial Intelligence p.27\n",
      "[9] 'AI Agents vs Agentic AI, A Conceptual Taxonomy Applications and Challenges' (2025) [Agentic AI] - Artificial Intelligence p.26\n",
      "[10] 'AI Agents vs Agentic AI, A Conceptual Taxonomy Applications and Challenges' (2025) [Agentic AI] - Artificial Intelligence p.8\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Filter by year - 2025 papers only (retry with updated metadata extraction)\n",
    "result = rag_tool._run(\"What are the latest advances in hierarchical reasoning and agentic AI?\", year=2025)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086a381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Small neural networks solve complex reasoning tasks through approaches like recursive hierarchical reasoning and deep supervision. These methods allow the networks to process information at different frequencies and improve their predictions iteratively. For instance, the Hierarchical Reasoning Model (HRM) uses two small networks that operate at high and low frequencies to generate different latent features, which are then used to refine answers. Additionally, deep supervision enhances performance by providing multiple feedback steps during training, allowing the model to learn from its mistakes and improve accuracy. The Tiny Recursive Model (TRM) further simplifies this process by using a single small network with fewer layers, achieving better generalization while minimizing overfitting. This combination of recursion and efficient architecture enables small networks to tackle complex reasoning tasks effectively.\n",
      "\n",
      "Sources:\n",
      "[1] 'Less is More, Recursive Reasoning with Tiny Networks' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.2\n",
      "[2] 'Less is More, Recursive Reasoning with Tiny Networks' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.10\n",
      "[3] 'Less is More, Recursive Reasoning with Tiny Networks' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.1\n",
      "[4] 'Less is More, Recursive Reasoning with Tiny Networks' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.10\n",
      "[5] 'Less is More, Recursive Reasoning with Tiny Networks' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.9\n",
      "[6] 'Less is More, Recursive Reasoning with Tiny Networks' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.7\n",
      "[7] 'Less is More, Recursive Reasoning with Tiny Networks' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.2\n",
      "[8] 'Less is More, Recursive Reasoning with Tiny Networks' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.1\n",
      "[9] 'Less is More, Recursive Reasoning with Tiny Networks' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.6\n",
      "[10] 'Less is More, Recursive Reasoning with Tiny Networks' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.4\n"
     ]
    }
   ],
   "source": [
    "# Query 4: Filter by topic - Hierarchical Reasoning Models\n",
    "result = rag_tool._run(\"How do small neural networks solve complex reasoning tasks?\", topic=\"Hierarchical Reasoning Models\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "542dd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16328\\1569910157.py:293: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self._qa_chain({\"question\": query, \"chat_history\": []})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16328\\1569910157.py:293: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self._qa_chain({\"question\": query, \"chat_history\": []})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: RAG (Retrieval-Augmented Generation) can be combined with fine-tuning for sentiment analysis by integrating the retrieval of relevant documents with the generation of sentiment classifications. The process typically involves the following steps:\n",
      "\n",
      "1. **Document Retrieval**: RAG first identifies relevant documents from a vector database based on a given query. This is done using a retriever component that finds the most pertinent information to provide context for the sentiment analysis task.\n",
      "\n",
      "2. **Combining Inputs**: The retrieved documents are then concatenated with the original query to form a combined input. This enriched input provides the language model with additional context that is relevant to the sentiment being analyzed.\n",
      "\n",
      "3. **Fine-Tuning**: The language model (such as Llama-2, Llama-3, GPT-3.5 Turbo, or GPT-4o Mini) is fine-tuned on domain-specific data, such as financial reviews. Techniques like LoRA (Low-Rank Adaptation) and quantization can be used to efficiently adapt the model to the specific nuances of the financial language.\n",
      "\n",
      "4. **Sentiment Classification Generation**: The fine-tuned model generates sentiment classification outputs based on the combined input, leveraging the context provided by the retrieved documents to enhance the accuracy and relevance of the sentiment predictions.\n",
      "\n",
      "5. **Ensemble Voting**: If multiple models are used, their outputs can be combined through majority voting to determine the final sentiment classification, ensuring robustness in the predictions.\n",
      "\n",
      "By integrating RAG with fine-tuning, the sentiment analysis process benefits from real-time context retrieval, leading to more precise and contextually relevant sentiment predictions.\n",
      "\n",
      "Sources:\n",
      "[1] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.10\n",
      "[2] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.25\n",
      "[3] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.10\n",
      "[4] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.20\n",
      "[5] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.1\n",
      "[6] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.3\n",
      "[7] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.35\n",
      "[8] 'HIRAG, Hierarchical Thought Instruction Tuning for RAG Models' (2025) [Hierarchical Reasoning Models] - Artificial Intelligence p.1\n",
      "[9] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.21\n",
      "[10] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.13\n"
     ]
    }
   ],
   "source": [
    "# Query 5: Question about RAG and sentiment analysis\n",
    "result = rag_tool._run(\"How can RAG be combined with fine-tuning for sentiment analysis?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e00bbfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The fine-tuning techniques discussed include:\n",
      "\n",
      "1. **Dynamic Modification of Hyper-parameters**: This involves adjusting parameters such as the number of epochs, batch size, and learning rate based on dataset properties.\n",
      "\n",
      "2. **Chain-of-Thought Reasoning and ReAct**: These methods are used to align model outputs with task objectives, allowing the models to capture nuanced sentiment cues.\n",
      "\n",
      "3. **Low-Rank Adaptation (LoRA)**: This technique allows for parameter-efficient fine-tuning by selectively updating a subset of model weights.\n",
      "\n",
      "4. **Quantisation**: Specifically, four-bit quantisation is used to reduce resource needs while retaining model performance.\n",
      "\n",
      "5. **Prompt Engineering**: Tailored prompts are designed to aid the models' interpretation processes, enhancing their ability to manage regional idioms and sentiment subtleties.\n",
      "\n",
      "6. **Few-shot Learning**: This approach is employed to help the models adapt to specific tasks with minimal examples.\n",
      "\n",
      "7. **Cross-Entropy Loss Function**: This function is used to reduce classification mistakes during the fine-tuning process.\n",
      "\n",
      "These techniques are aimed at improving the performance of language models in sentiment analysis, particularly in the context of financial reviews.\n",
      "\n",
      "Sources:\n",
      "[1] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.41\n",
      "[2] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.22\n",
      "[3] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.41\n",
      "[4] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.22\n",
      "[5] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.19\n",
      "[6] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.41\n",
      "[7] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.1\n",
      "[8] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.28\n",
      "[9] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.22\n",
      "[10] 'Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis' (2024) [Finetuning] - Artificial Intelligence p.3\n"
     ]
    }
   ],
   "source": [
    "# Query 6: Filter by year - 2024 papers only\n",
    "result = rag_tool._run(\"What fine-tuning techniques are discussed?\", year=2024)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93636e55",
   "metadata": {},
   "source": [
    "# Building the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70006fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research RAG Tool defined successfully!\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal, Dict\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Pydantic Models for Research RAG Tool (Agent Interface)\n",
    "# ============================================================================\n",
    "\n",
    "class SourceReference(BaseModel):\n",
    "    \"\"\"A reference to a source document.\"\"\"\n",
    "    paper_title: Optional[str] = Field(None, description=\"Title of the research paper\")\n",
    "    year: Optional[int] = Field(None, description=\"Publication year\")\n",
    "    topic: Optional[str] = Field(None, description=\"Topic area\")\n",
    "    subject: Optional[str] = Field(None, description=\"Subject area\")\n",
    "    page: Optional[int] = Field(None, description=\"Page number\")\n",
    "\n",
    "\n",
    "class ResearchProbeResponse(BaseModel):\n",
    "    \"\"\"Structured response from the research RAG tool.\"\"\"\n",
    "    topic: str = Field(..., description=\"The searched topic\")\n",
    "    category: str = Field(default=\"General\", description=\"Category: 'Agentic AI', 'Finetuning', 'Hierarchical Reasoning Models', 'General', or 'Not Found'\")\n",
    "    response: str = Field(..., description=\"Answer in markdown format\")\n",
    "    sources: List[SourceReference] = Field(default_factory=list, description=\"Source references\")\n",
    "    confidence: float = Field(default=0.0, ge=0.0, le=1.0, description=\"Confidence score (0-1)\")\n",
    "    query: str = Field(..., description=\"Original query\")\n",
    "    filters_applied: Dict[str, Any] = Field(default_factory=dict, description=\"Applied filters\")\n",
    "\n",
    "\n",
    "class ResearchProbeArgs(BaseModel):\n",
    "    \"\"\"Input arguments for the research paper probe tool.\"\"\"\n",
    "    query: str = Field(..., description=\"The research question to answer\")\n",
    "    topic: Optional[str] = Field(default=None, description=\"Topic filter: 'Agentic AI', 'Finetuning', 'Hierarchical Reasoning Models'\")\n",
    "    subject: Optional[str] = Field(default=None, description=\"Subject filter (e.g., 'Artificial Intelligence')\")\n",
    "    year: Optional[int] = Field(default=None, description=\"Publication year filter\", ge=1900, le=2100)\n",
    "    k: int = Field(default=10, ge=1, le=50, description=\"Number of documents to retrieve\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Core Tool Function (Direct metadata access - no parsing needed)\n",
    "# ============================================================================\n",
    "\n",
    "def _research_probe_fn(\n",
    "    query: str,\n",
    "    topic: Optional[str] = None,\n",
    "    subject: Optional[str] = None,\n",
    "    year: Optional[int] = None,\n",
    "    k: int = 10\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Search research papers and return structured response.\"\"\"\n",
    "    \n",
    "    filters_applied = {key: val for key, val in [(\"topic\", topic), (\"subject\", subject), (\"year\", year)] if val}\n",
    "    category = topic if topic else \"General\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize and get docs directly from vector store\n",
    "        rag_tool._initialize_components()\n",
    "        metadata_filter = rag_tool._build_metadata_filter(subject, topic, year)\n",
    "        \n",
    "        if metadata_filter:\n",
    "            docs = rag_tool._vectordb.similarity_search(query, k=k, filter=metadata_filter)\n",
    "        else:\n",
    "            docs = rag_tool._vectordb.similarity_search(query, k=k)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = rag_tool._generate_answer_from_docs(query, docs)\n",
    "        \n",
    "        # Build sources directly from document metadata (no parsing!)\n",
    "        sources = []\n",
    "        for doc in docs:\n",
    "            md = doc.metadata or {}\n",
    "            sources.append(SourceReference(\n",
    "                paper_title=md.get(\"paper_title\"),\n",
    "                year=md.get(\"year\"),\n",
    "                topic=md.get(\"topic\"),\n",
    "                subject=md.get(\"subject\"),\n",
    "                page=(md.get(\"page\", 0) + 1) if md.get(\"page\") is not None else None\n",
    "            ))\n",
    "            # Infer category from first source if not filtered\n",
    "            if not topic and md.get(\"topic\") and category == \"General\":\n",
    "                category = md.get(\"topic\")\n",
    "        \n",
    "        # Calculate confidence\n",
    "        confidence = min(1.0, len(sources) * 0.1 + (0.3 if answer and \"don't know\" not in answer.lower() else 0.0))\n",
    "        if not sources or \"don't know\" in answer.lower():\n",
    "            category = \"Not Found\" if not topic else category\n",
    "        \n",
    "        # Format markdown response\n",
    "        md_response = f\"## Answer\\n\\n{answer or '*No answer available.*'}\\n\"\n",
    "        if sources:\n",
    "            md_response += \"\\n## Sources\\n\\n\" + \"\\n\".join(\n",
    "                f\"{i}. *{s.paper_title}* ({s.year}) p.{s.page} [{s.topic}]\" \n",
    "                for i, s in enumerate(sources, 1) if s.paper_title\n",
    "            )\n",
    "        \n",
    "        return ResearchProbeResponse(\n",
    "            topic=query.split(\"?\")[0][:50], category=category, response=md_response,\n",
    "            sources=sources, confidence=confidence, query=query, filters_applied=filters_applied\n",
    "        ).model_dump()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return ResearchProbeResponse(\n",
    "            topic=query[:50], category=\"Not Found\", response=f\"## Error\\n\\n{str(e)}\",\n",
    "            sources=[], confidence=0.0, query=query, filters_applied=filters_applied\n",
    "        ).model_dump()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# StructuredTool for Agent Registration\n",
    "# ============================================================================\n",
    "\n",
    "research_probe = StructuredTool.from_function(\n",
    "    name=\"research_paper_probe\",\n",
    "    description=\"\"\"Search AI research papers to answer questions.\n",
    "\n",
    "Filters:\n",
    "- topic: 'Agentic AI', 'Finetuning', 'Hierarchical Reasoning Models'\n",
    "- year: Publication year (e.g., 2024, 2025)\n",
    "- subject: Subject area (e.g., 'Artificial Intelligence')\n",
    "\n",
    "Returns: topic, category, response (markdown), sources, confidence (0-1)\"\"\",\n",
    "    func=_research_probe_fn,\n",
    "    args_schema=ResearchProbeArgs,\n",
    ")\n",
    "\n",
    "print(\"Research RAG Tool defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9ef6584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Finetuning\n",
      "Confidence: 0.80\n",
      "\n",
      "## Answer\n",
      "\n",
      "The main techniques for fine-tuning LLMs include:\n",
      "\n",
      "1. **Dynamic Modification of Hyper-parameters**: This involves adjusting parameters such as the number of epochs, batch size, and learning rate based on dataset properties.\n",
      "\n",
      "2. **Use of Domain-Specific Data**: Fine-tuning is performed using datasets that reflect the specific language and cultural peculiarities of the target domain, such as customer reviews in the financial industry.\n",
      "\n",
      "3. **Targeted Prompts**: Prompts designed for Few-shot Learning, Chain-of-Thought reasoning, and ReAct are used to align model outputs with task objectives.\n",
      "\n",
      "4. **Low-Rank Adaptation (LoRA)**: This technique is employed to improve model performance in sentiment analysis tasks.\n",
      "\n",
      "5. **Quantisation**: This method is used to enhance the efficiency of the models during fine-tuning.\n",
      "\n",
      "6. **Cross-Entropy Loss Function**: This function is utilized to reduce classification mistakes during the training process.\n",
      "\n",
      "These techniques collectively enhance the models' ability to handle the specific language and sentiment nuances of the target domain.\n",
      "\n",
      "## Sources\n",
      "\n",
      "1. *Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis* (2024) p.19 [Finetuning]\n",
      "2. *Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis* (2024) p.10 [Finetuning]\n",
      "3. *Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis* (2024) p.22 [Finetuning]\n",
      "4. *Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis* (2024) p.41 [Finetuning]\n",
      "5. *Fine Tuning Retrieval Augmented Generation with an Auto Regressive Language Model for Sentiment Analysis* (2024) p.22 [Finetuning]\n"
     ]
    }
   ],
   "source": [
    "# Test the tool\n",
    "result = research_probe.invoke({\n",
    "    \"query\": \"What are the main techniques for fine-tuning LLMs?\",\n",
    "    \"topic\": \"Finetuning\",\n",
    "    \"year\": 2024,\n",
    "    \"k\": 5\n",
    "})\n",
    "\n",
    "print(f\"Category: {result['category']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "print(f\"\\n{result['response']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
