{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06534002",
   "metadata": {},
   "source": [
    "# Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851d22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated, TypedDict\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        want_tools = isinstance(result, AIMessage) and bool(getattr(result, \"tool_calls\", None))\n",
    "        return  want_tools\n",
    "\n",
    "    async def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = await self.model.ainvoke(messages) # aynchronous invoke\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    async def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = await self.tools[t['name']].ainvoke(t['args']) # aynchronous invoke\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c160615",
   "metadata": {},
   "source": [
    "# MCP Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e562c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d7b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPConnectionManager:\n",
    "    def __init__(self):\n",
    "        self.session = None\n",
    "        self.client = None\n",
    "        self._initialized = False\n",
    "    \n",
    "    async def initialize(self):\n",
    "        if self._initialized:\n",
    "            return self.session\n",
    "\n",
    "        # Create connection that stays alive\n",
    "        self.client = sse_client(\"http://127.0.0.1:8787/sse\")\n",
    "        read_stream, write_stream = await self.client.__aenter__()\n",
    "        self.session = ClientSession(read_stream, write_stream)\n",
    "        await self.session.__aenter__()\n",
    "        await self.session.initialize()\n",
    "        self._initialized = True\n",
    "        return self.session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1281ea3",
   "metadata": {},
   "source": [
    "# Workflow Demo: \n",
    "## Topic Not Found → arXiv Search → Download → Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6814cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to MCP Server\n",
      "✓ Loaded 3 tools: ['research_paper_probe', 'search_arxiv', 'download_paper']\n",
      "✓ Agent created with workflow system prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sse_reader\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 271, in __aiter__\n",
      "    async for part in self._httpcore_stream:\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 407, in __aiter__\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 403, in __aiter__\n",
      "    async for part in self._stream:\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 342, in __aiter__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 334, in __aiter__\n",
      "    async for chunk in self._connection._receive_response_body(**kwargs):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 203, in _receive_response_body\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 213, in _receive_event\n",
      "    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\mcp\\client\\sse.py\", line 72, in sse_reader\n",
      "    async for sse in event_source.aiter_sse():\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpx_sse\\_api.py\", line 39, in aiter_sse\n",
      "    async for line in self._response.aiter_lines():\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpx\\_models.py\", line 1031, in aiter_lines\n",
      "    async for text in self.aiter_text():\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpx\\_models.py\", line 1018, in aiter_text\n",
      "    async for byte_content in self.aiter_bytes():\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpx\\_models.py\", line 997, in aiter_bytes\n",
      "    async for raw_bytes in self.aiter_raw():\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpx\\_models.py\", line 1055, in aiter_raw\n",
      "    async for raw_stream_bytes in self.stream:\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpx\\_client.py\", line 176, in __aiter__\n",
      "    async for chunk in self._stream:\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 270, in __aiter__\n",
      "    with map_httpcore_exceptions():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\PT\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n"
     ]
    }
   ],
   "source": [
    "# Initialize MCP Connection and Create Agent with Tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Any\n",
    "import json\n",
    "\n",
    "# Connect to MCP Server\n",
    "mcp_manager = MCPConnectionManager()\n",
    "session = await mcp_manager.initialize()\n",
    "print(\"✓ Connected to MCP Server\")\n",
    "\n",
    "# Get tools from MCP\n",
    "tools_response = await session.list_tools()\n",
    "\n",
    "def create_mcp_tool_wrapper(tool_info):\n",
    "    \"\"\"Create a closure to capture the tool correctly\"\"\"\n",
    "    tool_name = tool_info.name\n",
    "    async def mcp_tool_wrapper(**kwargs):\n",
    "        result = await session.call_tool(tool_name, arguments=kwargs)\n",
    "        if result.content:\n",
    "            return result.content[0].text\n",
    "        return \"No content returned\"\n",
    "    return mcp_tool_wrapper\n",
    "\n",
    "# Convert MCP tools to LangChain StructuredTools\n",
    "tools = []\n",
    "for tool in tools_response.tools:\n",
    "    field_definitions = {}\n",
    "    if tool.inputSchema and \"properties\" in tool.inputSchema:\n",
    "        for prop_name, prop_info in tool.inputSchema[\"properties\"].items():\n",
    "            field_type = str if prop_info.get(\"type\") == \"string\" else Any\n",
    "            required = prop_name in tool.inputSchema.get(\"required\", [])\n",
    "            if required:\n",
    "                field_definitions[prop_name] = (field_type, Field(description=prop_info.get(\"description\", \"\")))\n",
    "            else:\n",
    "                field_definitions[prop_name] = (Optional[field_type], Field(default=None, description=prop_info.get(\"description\", \"\")))\n",
    "    \n",
    "    ArgsModel = type(f\"{tool.name}Args\", (BaseModel,), {\n",
    "        \"__annotations__\": {k: v[0] for k, v in field_definitions.items()}, \n",
    "        **{k: v[1] for k, v in field_definitions.items()}\n",
    "    })\n",
    "    \n",
    "    structured_tool = StructuredTool(\n",
    "        name=tool.name,\n",
    "        description=tool.description,\n",
    "        coroutine=create_mcp_tool_wrapper(tool),\n",
    "        args_schema=ArgsModel\n",
    "    )\n",
    "    tools.append(structured_tool)\n",
    "\n",
    "print(f\"✓ Loaded {len(tools)} tools: {[t.name for t in tools]}\")\n",
    "\n",
    "# Create LLM and Agent\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "workflow_system_prompt = \"\"\"You are a research assistant that helps users find information from academic papers.\n",
    "\n",
    "You have access to these tools:\n",
    "1. research_paper_probe - Search the local RAG knowledge base of research papers\n",
    "2. search_arxiv - Search arXiv for new academic papers  \n",
    "3. download_paper - Download papers from arXiv and add them to the knowledge base\n",
    "\n",
    "WORKFLOW:\n",
    "1. When a user asks about a topic, FIRST use research_paper_probe to check the local knowledge base.\n",
    "2. If NO relevant results are found (low confidence or empty sources), inform the user and ASK if they want you to search arXiv for papers on this topic.\n",
    "3. If the user says yes, use search_arxiv to find relevant papers. Present the results as a numbered list with title, authors, year, and a brief abstract summary.\n",
    "4. Ask the user which papers (up to 3) they would like to add to the collection.\n",
    "5. When the user selects papers, use download_paper for each selected paper to download and index them.\n",
    "6. After downloading, use research_paper_probe again to answer the original question using the newly added papers.\n",
    "\n",
    "Always cite your sources with paper titles and years.\"\"\"\n",
    "\n",
    "agent = Agent(llm, tools, system=workflow_system_prompt)\n",
    "print(\"✓ Agent created with workflow system prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "260eab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'research_paper_probe', 'args': {'query': 'latest advances in using LLMs and AI agents for drug discovery'}, 'id': 'call_LkIKMruscTI9Bk8I1mWrmP1u', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "============================================================\n",
      "WORKFLOW STEP 1: Initial Query\n",
      "============================================================\n",
      "User: What are the latest advances in using LLMs and AI agents for drug discovery?\n",
      "------------------------------------------------------------\n",
      "Agent: I couldn't find specific papers on the latest advances in using LLMs (Large Language Models) and AI agents for drug discovery in the local knowledge base. Would you like me to search arXiv for recent academic papers on this topic?\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Ask about a topic NOT in the knowledge base\n",
    "query = \"What are the latest advances in using LLMs and AI agents for drug discovery?\"\n",
    "messages = [HumanMessage(content=query)]\n",
    "\n",
    "result = await agent.graph.ainvoke({\"messages\": messages})\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Initial Query (Topic Not Found)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"User: {query}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcf1ecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'search_arxiv', 'args': {'query': 'LLM agents drug discovery', 'subject': 'Artificial Intelligence'}, 'id': 'call_8XvtXDmEdNvi3TlGnx0ffyqH', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "============================================================\n",
      "WORKFLOW STEP 2: Search arXiv\n",
      "============================================================\n",
      "User: Yes, please search for papers on LLM agents drug discovery\n",
      "------------------------------------------------------------\n",
      "Agent: Here are some recent academic papers related to LLMs and AI agents in the context of drug discovery:\n",
      "\n",
      "1. **[RAG-Enhanced Collaborative LLM Agents for Drug Discovery](http://arxiv.org/abs/2502.17506v3)**  \n",
      "   - **Authors:** Namkyeong Lee, Edward De Brouwer, Ehsan Hajiramezanali, Tommaso Biancalani, Chanyoung Park  \n",
      "   - **Year:** 2025  \n",
      "   - **Abstract:** This paper discusses the potential of large language models (LLMs) in drug discovery and addresses the challenges posed by the specialized nature of biochemical data, which often requires costly domain-specific fine-tuning. The authors propose RAG-enhanced collaborative LLM agents to better integrate vast amounts of scientific data generated through research.  \n",
      "   - [Download PDF](https://arxiv.org/pdf/2502.17506v3)\n",
      "\n",
      "2. **[Creative Problem Solving in Artificially Intelligent Agents: A Survey and Framework](http://arxiv.org/abs/2204.10358v1)**  \n",
      "   - **Authors:** Evana Gizzi, Lakshmi Nair, Sonia Chernova, Jivko Sinapov  \n",
      "   - **Year:** 2022  \n",
      "   - **Abstract:** This survey explores methods for creative problem solving in AI, relevant in contexts where they must adapt existing knowledge to new situations, such as in drug discovery where novel problems arise.  \n",
      "   - [Download PDF](https://arxiv.org/pdf/2204.10358v1)\n",
      "\n",
      "3. **[The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence](http://arxiv.org/abs/2110.01831v1)**  \n",
      "   - **Authors:** Michael Timothy Bennett, Yoshihiro Maruyama  \n",
      "   - **Year:** 2021  \n",
      "   - **Abstract:** This paper discusses different approaches to construct an Artificial Scientist, which could be essential for advancing areas like drug discovery through AI.  \n",
      "   - [Download PDF](https://arxiv.org/pdf/2110.01831v1)\n",
      "\n",
      "Would you like to add any of these papers to your collection? You can choose up to 3 papers.\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: User says YES → Agent searches arXiv\n",
    "messages = result['messages'] + [HumanMessage(content=\"Yes, please search for papers on LLM agents drug discovery\")]\n",
    "\n",
    "result = await agent.graph.ainvoke({\"messages\": messages})\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: Search arXiv\")\n",
    "print(\"=\" * 60)\n",
    "print(\"User: Yes, please search for papers on LLM agents drug discovery\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecd8157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'download_paper', 'args': {'pdf_url': 'https://arxiv.org/pdf/2502.17506v3', 'title': 'RAG-Enhanced Collaborative LLM Agents for Drug Discovery', 'year': 2025, 'subject': 'Artificial Intelligence', 'topic': None}, 'id': 'call_dPqTJcElUsYpF5Z1CimvHQOm', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "============================================================\n",
      "WORKFLOW STEP 3: Download & Index Paper\n",
      "============================================================\n",
      "User: Please add paper #1 (RAG-Enhanced Collaborative LLM Agents for Drug Discovery) to the collection.\n",
      "------------------------------------------------------------\n",
      "Agent: The paper **\"RAG-Enhanced Collaborative LLM Agents for Drug Discovery\"** (2025) has been successfully added to the collection. You can find it in your papers directory:\n",
      "\n",
      "- **File Path:** `C:\\Users\\User\\Desktop\\llms\\Project\\Research Assistant Multi Agent System\\Tools Server\\RAG SETUP\\Papers\\Artificial Intelligence\\Uncategorized\\RAG-Enhanced Collaborative LLM Agents for Drug Discovery - 2025.pdf`\n",
      "\n",
      "If you need further assistance or want to explore more topics, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: User selects paper → Agent downloads and indexes it\n",
    "messages = result['messages'] + [HumanMessage(content=\"Please add paper #1 to the collection.\")]\n",
    "\n",
    "result = await agent.graph.ainvoke({\"messages\": messages})\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: Download & Index Paper\")\n",
    "print(\"=\" * 60)\n",
    "print(\"User: Please add paper #1 to the collection.\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2003f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4: Query Newly Indexed Paper\n",
      "============================================================\n",
      "Query: How do RAG-enhanced LLM agents work for drug discovery?\n",
      "Confidence: 0.8\n",
      "------------------------------------------------------------\n",
      "## Answer\n",
      "\n",
      "RAG-enhanced LLM agents work for drug discovery by dynamically retrieving information from biomedical knowledge bases and contextualizing query molecules. They integrate relevant evidence to generate responses without the need for domain-specific fine-tuning. This approach allows the agents to adapt to new insights and complex scientific questions in real-time, addressing challenges such as data heterogeneity, ambiguity, and multi-source integration. The framework leverages the collaboration of multiple LLM agents to improve the effectiveness of tasks like drug-target prediction, molecular captioning, and biological activity prediction, outperforming both general-purpose and domain-specific LLMs as well as traditional deep learning methods.\n",
      "\n",
      "## Sources\n",
      "\n",
      "1. *RAG-Enhanced Collaborative LLM Agents for Drug Discovery* (2025) p.1 [Drug Discovery]\n",
      "2. *RAG-Enhanced Collaborative LLM Agents for Drug Discovery* (2025) p.1 [Drug Discovery]\n",
      "3. *RAG-Enhanced Collaborative LLM Agents for Drug Discovery* (2025) p.1 [Drug Discovery]\n",
      "4. *RAG-Enhanced Collaborative LLM Agents for Drug Discovery* (2025) p.2 [Drug Discovery]\n",
      "5. *RAG-Enhanced Collaborative LLM Agents for Drug Discovery* (2025) p.2 [Drug Discovery]\n",
      "------------------------------------------------------------\n",
      "Sources:\n",
      "  - RAG-Enhanced Collaborative LLM Agents for Drug Discovery (2025) p.1\n",
      "  - RAG-Enhanced Collaborative LLM Agents for Drug Discovery (2025) p.1\n",
      "  - RAG-Enhanced Collaborative LLM Agents for Drug Discovery (2025) p.1\n",
      "  - RAG-Enhanced Collaborative LLM Agents for Drug Discovery (2025) p.2\n",
      "  - RAG-Enhanced Collaborative LLM Agents for Drug Discovery (2025) p.2\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Query the newly added paper\n",
    "\n",
    "probe_result = await session.call_tool(\"research_paper_probe\", arguments={\n",
    "    \"query\": \"How do RAG-enhanced LLM agents work for drug discovery?\",\n",
    "    \"k\": 5\n",
    "})\n",
    "response = json.loads(probe_result.content[0].text)\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: Query Newly Indexed Paper\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: How do RAG-enhanced LLM agents work for drug discovery?\")\n",
    "print(f\"Confidence: {response['confidence']}\")\n",
    "print(\"-\" * 60)\n",
    "print(response['response'])\n",
    "print(\"-\" * 60)\n",
    "print(\"Sources:\")\n",
    "for src in response['sources']:\n",
    "    print(f\"  - {src['paper_title']} ({src['year']}) p.{src['page']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839dc18",
   "metadata": {},
   "source": [
    "# Getting the requirements from the kernel environmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee19846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fastapi\n",
      "Version: 0.116.1\n",
      "------------------------------\n",
      "Name: uvicorn\n",
      "Version: 0.35.0\n",
      "------------------------------\n",
      "Name: langchain\n",
      "Version: 0.3.27\n",
      "------------------------------\n",
      "Name: langchain-core\n",
      "Version: 0.3.72\n",
      "------------------------------\n",
      "Name: langchain-openai\n",
      "Version: 0.3.28\n",
      "------------------------------\n",
      "Name: langgraph\n",
      "Version: 0.6.3\n",
      "------------------------------\n",
      "Name: openai\n",
      "Version: 1.100.1\n",
      "------------------------------\n",
      "Name: chromadb\n",
      "Version: 1.0.17\n",
      "------------------------------\n",
      "Name: pydantic\n",
      "Version: 2.11.7\n",
      "------------------------------\n",
      "Name: python-dotenv\n",
      "Version: 1.1.1\n",
      "------------------------------\n",
      "Name: httpx\n",
      "Version: 0.28.1\n",
      "------------------------------\n",
      "Name: arxiv\n",
      "Version: 2.3.1\n",
      "------------------------------\n",
      "Name: PyMuPDF\n",
      "Version: 1.26.3\n",
      "------------------------------\n",
      "Name: mcp\n",
      "Version: 1.12.4\n",
      "------------------------------\n",
      "Name: pypdf\n",
      "Version: 6.0.0\n",
      "------------------------------\n",
      "Name: aiohttp\n",
      "Version: 3.12.14\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    \"fastapi\", \"uvicorn\", \"langchain\", \"langchain-core\", \"langchain-openai\",\n",
    "    \"langgraph\", \"openai\", \"chromadb\", \"pydantic\", \"python-dotenv\",\n",
    "    \"httpx\", \"arxiv\", \"pymupdf\", \"mcp\", \"pypdf\", \"aiohttp\"\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    result = subprocess.run([\"pip\", \"show\", pkg], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if line.startswith('Name:') or line.startswith('Version:'):\n",
    "                print(line)\n",
    "        print(\"-\" * 30)\n",
    "    else:\n",
    "        print(f\"{pkg}: Not installed\")\n",
    "        print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
